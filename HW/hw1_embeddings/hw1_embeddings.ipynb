{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "318498f4-7f14-42a7-bbfd-6ef83d3349b7",
   "metadata": {},
   "source": [
    "# INFO 159/259"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9037f2a-d06a-4c4c-ba71-d571e5873c01",
   "metadata": {},
   "source": [
    "# <center> Homework 1: Word Embeddings </center>\n",
    "<center> Due: February 3, 2026 @ 11:59pm </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47db1e18-852a-46ee-8ae9-c9b1b8682320",
   "metadata": {},
   "source": [
    "# HW1: Word Embeddings\n",
    "\n",
    "In this homework, you will implement _word2vec_ with skip-grams and negative sampling, training on a small slice of Wikipedia data.\n",
    "\n",
    "*Learning objectives*:\n",
    "- Understand the implementation details of _word2vec_\n",
    "- Gain familiarity with `numpy` for matrix math\n",
    "- Gain familiarity with training a classifier using stochastic gradient descent.\n",
    "\n",
    "You may want to consult SLP chapter 5 (_Embeddings_) as a reference for the implementation. This homework is designed to run on the CPU only, so if you are using Google Colab, you may want to ensure that your CPU is selected (under `Runtime > Change runtime type` in the top bar) so that you save your GPU allocation for later assignments in the semester."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a59d00f-8341-414e-98cd-485cdcd78f05",
   "metadata": {},
   "source": [
    "# download the dataset we will be using\n",
    "!wget https://github.com/dbamman/nlp-course/raw/refs/heads/main/HW/data/en_wiki_sample.txt -O en_wiki_sample.txt"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c695c9b0-353d-4194-8f2f-e94a665c2b97",
   "metadata": {},
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32448ff3-a870-4621-9f9d-a1e83f3bc641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "# nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6563cd-75ec-472f-bc73-d0af6fd4e68e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Data loading\n",
    "\n",
    "We will begin by loading and tokenizing the data. The file contains a list of paragraphs from Wikipeda, separated by newlines. Because each document (a paragraph) is sampled independently, we want to maintain the document boundaries when we sample contexts later.\n",
    "\n",
    "Inside `FileDataLoader`:\n",
    "- `idx2vocab` is a list of unique word types\n",
    "- `vocab2idx` is a dict mapping from a word type to its index in `idx2vocab`\n",
    "- `word_freqs` is a dict mapping from a word type to its frequency in the corpus\n",
    "\n",
    "You should implement:\n",
    "1. The `negative_sample_weights()` function\n",
    "\n",
    "   This function should calculate the weighted sample probabilities for each of the words in our vocabulary.\n",
    "   Recall SLP3 eq. 5.19:\n",
    "    $$\n",
    "     P_{\\alpha}(w) = \\frac{\\text{count}(w)^{\\alpha}}{\\sum_{w'}\\text{count}(w')^{\\alpha}}\n",
    "    $$\n",
    "   We calculate and store the sample weights to save time when generating contexts later.\n",
    "3. The `negative_sample()` function\n",
    "\n",
    "   This function should sample `num_samples` negative context words given a target word. Recall from SLP3 5.5.2\n",
    "   > A noise word is a random word from the lexicon, **constrained not to be the target word $w$**. (_emph added_)\n",
    "\n",
    "   So, when sampling, you will want to copy the original `.sample_weights` numpy array, set the probability of the target word to 0, and renormalize the weights before sampling.\n",
    "\n",
    "   You may want to consult the numpy documentation for [`numpy.random.Generator.choice()`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html#numpy.random.Generator.choice). We have instantiated a random generator for your convenience in `self.rng`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe878ae3-b7ff-45a3-9a85-1dcb2a9b001e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "_Learning objectives_:\n",
    "> - Understand the implementation details of _word2vec_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7411592b-c7af-4fc2-a332-ccd22c87c26e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "corpus_path = \"./en_wiki_sample.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "df7830a5-b148-4002-b927-73f7bcf3d005",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "class FileDataLoader():\n",
    "    def __init__(self, filepath, negative_sample_alpha=0.75, min_threshold=5):\n",
    "        self.negative_sample_alpha = negative_sample_alpha\n",
    "        self.min_threshold = min_threshold\n",
    "\n",
    "        self.tokenized_documents = self.load_data(filepath)\n",
    "        self.word_freqs = self.get_word_freqs(self.tokenized_documents)\n",
    "\n",
    "        # replace words that appear fewer than min_threshold times with an [UNK] token\n",
    "        for word, freq in list(self.word_freqs.items()):\n",
    "            if freq < min_threshold:\n",
    "                self.word_freqs[\"[UNK]\"] += freq\n",
    "                del self.word_freqs[word]\n",
    "\n",
    "        self.idx2vocab = list(self.word_freqs.keys())\n",
    "        self.vocab2idx = {word: index for index, word in enumerate(self.idx2vocab)}\n",
    "\n",
    "        # set up a random number generator we can use for sampling\n",
    "        self.rng = np.random.default_rng(159259)\n",
    "        self.sample_weights = self.negative_sample_weights(alpha=negative_sample_alpha)\n",
    "\n",
    "        ...\n",
    "\n",
    "    def tokenize_and_lowercase(self, doc):\n",
    "        \"\"\"Tokenize a doc and lowercase all the words.\"\"\"\n",
    "        return [word.lower() for word in word_tokenize(doc)]\n",
    "\n",
    "    def get_word_freqs(self, tokenized_documents):\n",
    "        \"\"\"Return a dictionary mapping each word to its frequency.\"\"\"\n",
    "        return Counter(itertools.chain.from_iterable(tokenized_documents))\n",
    "\n",
    "    def load_data(self, filepath):\n",
    "        return [self.tokenize_and_lowercase(doc) for doc in tqdm(open(corpus_path, \"r\").readlines())]\n",
    "\n",
    "    def negative_sample_weights(self, alpha):\n",
    "        \"\"\"Calculate the weighted probabilities of each word.\n",
    "\n",
    "        Return a (v,)-shaped numpy array, where v is the size of the vocabulary.\n",
    "        \"\"\"\n",
    "        freq_arr = np.array(list(self.word_freqs.values()))\n",
    "        num = freq_arr**alpha\n",
    "        den = np.sum(num)\n",
    "        wt_prob = num/den\n",
    "        return wt_prob\n",
    "\n",
    "    def negative_sample(self, target_word_idx, num_samples):\n",
    "        \"\"\"Sample num_samples noise words from the lexicon that is not the target word.\n",
    "    \n",
    "        The sample probabilities should be proportional to their weighted unigram probability if the target word probability is set to 0.\n",
    "\n",
    "        Return a (num_samples,)-shaped numpy array of sampled indices.\n",
    "        \"\"\"\n",
    "        freq_arr = np.array(self.sample_weights)\n",
    "        freq_arr[target_word_idx] = 0\n",
    "        freq_arr_norm = freq_arr/sum(freq_arr)\n",
    "        negative_idx = np.random.choice(len(freq_arr_norm), size=num_samples, p=freq_arr_norm)\n",
    "        return negative_idx\n",
    "\n",
    "    def sample_contexts(self, window_size, sample_k):\n",
    "        for doc in self.tokenized_documents:\n",
    "            if len(doc) < (2 * window_size) + 1:\n",
    "                # the doc is too short for our desired window size; we skip it\n",
    "                continue\n",
    "            for word_idx in range(window_size, len(doc) - window_size):\n",
    "                target_word_idx = self.vocab2idx[doc[word_idx]] if doc[word_idx] in self.vocab2idx else self.vocab2idx[\"[UNK]\"]\n",
    "                # sample positive words from the window\n",
    "                positive_word_idxs = np.array([\n",
    "                    self.vocab2idx[word] if word in self.vocab2idx else self.vocab2idx[\"[UNK]\"] for word in doc[word_idx - window_size:word_idx] + doc[word_idx + 1:word_idx + 1 + window_size]\n",
    "                    \n",
    "                ])\n",
    "                # sample len(positive_word_idxs) * sample_k number of negative words\n",
    "                negative_word_idxs = self.negative_sample(target_word_idx, sample_k * len(positive_word_idxs))\n",
    "                yield (target_word_idx, positive_word_idxs, negative_word_idxs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ee0d413e-3180-4601-9e6a-3ce2c9cbe07b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 100000/100000 [00:17<00:00, 5765.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# this should take roughly 30 seconds\n",
    "dataloader = FileDataLoader(corpus_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4c9e9e-9515-4283-92e6-2aad6065e023",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Quick check**: The unweighted probability for \"the\" should be 0.063; the weighted probability should be 0.016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1d400540-0f58-4194-8256-fbcc817a0ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "511984"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader.word_freqs['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9b355a6b-a59b-41bf-84ff-caf125c793ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unweighted probability for `the`: \t\t0.063\n",
      "Weighted (alpha=0.75) probability for `the`: \t0.016\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unweighted probability for `the`: \\t\\t{dataloader.word_freqs['the'] / sum(dataloader.word_freqs.values()):.3f}\")\n",
    "print(f\"Weighted (alpha=0.75) probability for `the`: \\t{dataloader.sample_weights[dataloader.vocab2idx['the']]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "82045973-aa69-4fce-b055-db8d9bbdd47e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Counter' object has no attribute 'total'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[123], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnweighted probability for `the`: \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdataloader\u001b[38;5;241m.\u001b[39mword_freqs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mdataloader\u001b[38;5;241m.\u001b[39mword_freqs\u001b[38;5;241m.\u001b[39mtotal()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeighted (alpha=0.75) probability for `the`: \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdataloader\u001b[38;5;241m.\u001b[39msample_weights[dataloader\u001b[38;5;241m.\u001b[39mvocab2idx[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Counter' object has no attribute 'total'"
     ]
    }
   ],
   "source": [
    "print(f\"Unweighted probability for `the`: \\t\\t{dataloader.word_freqs['the'] / dataloader.word_freqs.total():.3f}\")\n",
    "print(f\"Weighted (alpha=0.75) probability for `the`: \\t{dataloader.sample_weights[dataloader.vocab2idx['the']]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ac31be-c0cf-4fe3-a6e1-5494f4c596d2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Setting up the model\n",
    "\n",
    "The word2vec model consists of two matrices: the target (or input) embedding and the context (or output) embedding. We set those up here.\n",
    "\n",
    "You should implement:\n",
    "- The `nearest_neighbors()` function\n",
    "\n",
    "  This given a $d$-dimensional $\\vec{v}$ and a $(v \\times d)$-dimensional matrix $M$ of vectors to query against, we want to calculate the cosine similarity of $\\vec{v}$ with each row of $M$ and return the indices (and the corresponding similarities) of the most similar rows in $M$.\n",
    "\n",
    "  As a reminder, the cosine similarity of two vectors $\\vec{a}$ and $\\vec{b}$ is\n",
    "  $$\n",
    "    \\text{cosine\\_sim}(\\vec{a}, \\vec{b}) = \\frac{\\vec{a} \\cdot \\vec{b}}{\\|{\\vec{a}}\\|\\|\\vec{b}\\|}\n",
    "  $$\n",
    "\n",
    "  This is derived from one of the formulations for the dot product:\n",
    "  $$\n",
    "    \\vec{a} \\cdot \\vec{b} = \\|\\vec{a}\\| \\|\\vec{b}\\| \\cos({\\theta})\n",
    "  $$\n",
    "\n",
    "  $\\|\\vec{a}\\|$ denotes the $l_2$-norm of a vector, or its magnitude.\n",
    "\n",
    "  You might want to consult the numpy documentation for [`numpy.matmul`](https://numpy.org/doc/2.1/reference/generated/numpy.matmul.html), [`numpy.argsort`](https://numpy.org/doc/2.1/reference/generated/numpy.argsort.html#numpy-argsort), and [`numpy.linalg.norm`](https://numpy.org/doc/2.1/reference/generated/numpy.linalg.norm.html)\n",
    "\n",
    "\n",
    "_Learning objectives_:\n",
    "> - Gain familiarity with `numpy` for matrix math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "786c582e-c88c-45d0-944f-3cf2b6e165bf",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "class Word2Vec():\n",
    "    def __init__(self, dataloader, hidden_dim=100):\n",
    "        self.dataloader = dataloader\n",
    "        self.vocab_size = len(self.dataloader.idx2vocab)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        np.random.seed(159259)\n",
    "        # We initialize the model weights to be uniformly randomly distributed and centered around 0.\n",
    "        self.target_embs = (np.random.random((self.vocab_size, hidden_dim)) - 0.5) / hidden_dim\n",
    "        self.context_embs = (np.random.random((self.vocab_size, hidden_dim)) - 0.5) / hidden_dim\n",
    "\n",
    "    def nearest_neighbors(self, query_vector, vectors, n=10):\n",
    "        \"\"\"Finds the `n` indices of the rows in `vectors` that have the highest cosine similarity to `query_vector`.\n",
    "\n",
    "        query_vector: (d,)-shaped numpy array\n",
    "        vectors: (v, d)-shaped numpy array\n",
    "        n: int\n",
    "        \n",
    "        Return a tuple of (indices, similarities), where both are (n,)-shaped ndarrays.\n",
    "        \"\"\"\n",
    "        query_vector_norm = (query_vector/np.sqrt((query_vector**2).sum()))[:, np.newaxis]\n",
    "        vectors_norm = vectors/(np.sqrt((vectors**2).sum(axis=1))[:, np.newaxis])\n",
    "\n",
    "        vector_sim = ((vectors_norm)@(query_vector_norm)).ravel()\n",
    "\n",
    "        idx = np.argsort(vector_sim)[-n:][::-1]\n",
    "        sims = vector_sim[idx]\n",
    "        \n",
    "        return (idx, sims)\n",
    "\n",
    "    def print_nearest_neighbors(self, word, n=5):\n",
    "        \"\"\"Prints the `n` nearest neighbors for a word using the context embeddings.\n",
    "\n",
    "        word: str\n",
    "\n",
    "        Return None\n",
    "        \"\"\"\n",
    "        query_vector = self.context_embs[self.dataloader.vocab2idx[word]]\n",
    "        closest_inds, similarities = self.nearest_neighbors(query_vector, self.context_embs, n)\n",
    "        words = [self.dataloader.idx2vocab[ind] for ind in closest_inds]\n",
    "\n",
    "        print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "c86a648f-9671-4f85-8644-a095f9dfe708",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c8e4f-6f57-4e1a-a198-4bb6a70edc2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf8bc322-4dad-4810-b4c1-eea1298c4042",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Quick check**: you can check your function against this toy example. The output should be:\n",
    "\n",
    "- `(array([4, 5, 0, 6, 3]), array([0.91347529, 0.87409283, 0.84518755, 0.83396453, 0.8111933 ]))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "c3a58b3e-b51b-423e-bf1e-71764ef0d9fb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([4, 5, 0, 6, 3]), array([0.91347529, 0.87409283, 0.84518755, 0.83396453, 0.8111933 ]))\n"
     ]
    }
   ],
   "source": [
    "def quick_check():\n",
    "    np.random.seed(159259)\n",
    "    query_vec = np.random.random(size=(5,))\n",
    "    other_vecs = np.random.random(size=(10, 5))\n",
    "    print(w2v_model.nearest_neighbors(query_vec, other_vecs, n=5))\n",
    "\n",
    "quick_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6196d4-bf38-4de1-b3fd-6ca986056a7a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Quick check**: the nearest neighbors for \"the\" should be random at this point; if you did not edit the `__init__` function, the nearest neighbors should be:\n",
    "\n",
    "- `['the', 'asian', 'habilitation', 'toward', 'capacity-building']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "bcd4e8e8-10a2-4a02-8fcd-cf05d63624b0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'asian', 'habilitation', 'toward', 'capacity-building']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n_/rj77m3n95lnctbnc2jkwbhw00000gn/T/ipykernel_17732/3844574401.py:24: RuntimeWarning: divide by zero encountered in matmul\n",
      "  vector_sim = ((vectors_norm)@(query_vector_norm)).ravel()\n",
      "/var/folders/n_/rj77m3n95lnctbnc2jkwbhw00000gn/T/ipykernel_17732/3844574401.py:24: RuntimeWarning: overflow encountered in matmul\n",
      "  vector_sim = ((vectors_norm)@(query_vector_norm)).ravel()\n",
      "/var/folders/n_/rj77m3n95lnctbnc2jkwbhw00000gn/T/ipykernel_17732/3844574401.py:24: RuntimeWarning: invalid value encountered in matmul\n",
      "  vector_sim = ((vectors_norm)@(query_vector_norm)).ravel()\n"
     ]
    }
   ],
   "source": [
    "w2v_model.print_nearest_neighbors(\"the\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dab544-e659-434e-8ef0-74b620cccc9d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Setting up the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f9f4b2-9e29-4d62-8d5e-0e04797fd2b1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Calculating gradients\n",
    "\n",
    "To update the weights using gradient descent, we have to find the partial derivatives of the loss with respect to the parameters. You can find the loss function and its partial derivatives in SLP 5.5.2 (eqs. 5.22 - 5.24); we've also reproduced them for you below. While we give you the derivatives, it can be a good exercise to try to derive them yourself!\n",
    "\n",
    "These rely on the sigmoid function, which we've implemented for you as an example.\n",
    "\n",
    "You should implement:\n",
    "- `loss_fn`\n",
    "- `c_pos_grad`\n",
    "- `c_neg_grad`\n",
    "- `w_grad`\n",
    "\n",
    "In each of these functions, you should expect:\n",
    "- `w` to be a `d`-dimensional vector,\n",
    "- `c_pos` to be a `(n_pos, d)`-dimensional matrix (where `n_pos` is the number of positive context examples)\n",
    "- `c_neg` to be a `(n_neg, d)`-dimensional matrix (where `n_neg` is the number of negative context examples)\n",
    "\n",
    "As a reminder, the sigmoid function is defined as\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "For filling out the rest of the functions, you may want to use [`np.log`](https://numpy.org/devdocs/reference/generated/numpy.log.html#numpy.log), [`np.sum`](https://numpy.org/devdocs/reference/generated/numpy.sum.html), [`np.newaxis`](https://numpy.org/devdocs/reference/constants.html#numpy.newaxis), [`np.matmul`](https://numpy.org/devdocs/reference/generated/numpy.matmul.html#numpy-matmul), and of course, the `sigmoid` function that we have implemented for you."
   ]
  },
  {
   "cell_type": "raw",
   "id": "596d9ed3-4333-4a26-bafa-b4c4fb28c875",
   "metadata": {},
   "source": [
    "!{sys.executable} -m pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "ca4fe294-5650-4bca-a43a-787ac3df9125",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# we wrap these functions in the @njit decorator to speed up calculations\n",
    "# using just-in-time compilation\n",
    "# you don't have to worry about this\n",
    "from numba import njit\n",
    "\n",
    "@njit\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "f07f23e3-aea8-4e7e-8fd9-7e163082463a",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def loss_fn(w, c_pos, c_neg):\n",
    "    pos_prob = sigmoid(c_pos@w[:, np.newaxis])\n",
    "    neg_prob = sigmoid(-c_neg@w[:, np.newaxis])\n",
    "    loss = -(np.log(pos_prob).sum() + np.log(neg_prob).sum())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc966ebd-ebcb-4d81-9851-a7974a14e642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "62d4d438-55d4-4679-bcd6-085eb8a4e0b7",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def c_pos_grad(w, c_pos):\n",
    "    grad = ((sigmoid(c_pos@w[:, np.newaxis])-1)@w[:, np.newaxis].T)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f02534b-bd55-47e9-8b58-7e00aead0e40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "b76a8d2c-f760-48c4-bef3-b39de087d9b2",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def c_neg_grad(w, c_neg):\n",
    "    grad = ((sigmoid(c_neg@w[:, np.newaxis]))@w[:, np.newaxis].T)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "e99ed4d6-fff8-4984-9a02-4da80a63eb97",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def w_grad(w, c_pos, c_neg):\n",
    "    pos_grad = (c_pos.T@(sigmoid(c_pos@w[:, np.newaxis])-1))\n",
    "    neg_grad = (c_neg.T@(sigmoid(c_neg@w[:, np.newaxis])))\n",
    "    grad = (pos_grad+neg_grad).ravel()\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "raw",
   "id": "378bf745-1ef7-4c69-bc19-e802b7aacc34",
   "metadata": {},
   "source": [
    "np.random.seed(159259)\n",
    "\n",
    "w = np.random.random((5,))\n",
    "c_pos = np.random.random((2, 5))\n",
    "c_neg = np.random.random((4, 5))\n",
    "\n",
    "eps = 1e-5\n",
    "\n",
    "print(\"c_pos_grad:\", w_grad(w, c_pos, c_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47cc884-ee4d-440f-871f-0d8d8831aea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166e6008-be92-48bc-b1f3-fd83578833aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd707bf6-2271-41ae-a2d8-219b69fe5c3f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**(Not so) Quick check**: We can check the correctness of the loss function and gradient calculations by numerically approximating the gradients using neighboring points and seeing if they match up. Recall from your calculus class:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx} f(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x - h)}{2h}\n",
    "$$\n",
    "\n",
    "We implement this in the `approximate_gradient` function so that we can estimate the local gradient and see if the closed-form solution that you implemented in the functions above are accurate. However, we never numerically approximate the gradient during training because we have a closed-form solution that is both more accurate and more efficient to calculate.\n",
    "\n",
    "> **Aside**: In this assignment, we have you manually calculate the loss and gradients. If you have taken other deep learning classes, you may have experience with libraries like Pytorch, which implement automatic differentiation so that you can just specify the loss function and not have to work out the gradients manually.\n",
    ">\n",
    "> These libraries _don't_ use numerical approximation for the gradients. Instead, they rely on the chain rule:\n",
    ">\n",
    "> $$\n",
    "    \\frac{d}{dx} f(g(x)) = f'(g(x)) g'(x)\n",
    "  $$\n",
    "> As long as all of the functions you apply to an input are differentiable, and the closed-form derivatives are known (which they often are, since most functions break down into basic differentiable operations like addition, multiplication, or exponentiation), the library can construct a graph to track all of the applications of the functions and calculate the partial derivatives using this graph.\\\n",
    ">\n",
    "> You can read more about this in the [Pytorch autograd tutorial](https://docs.pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#computational-graph).\n",
    "\n",
    "Your loss should be roughly 8.05; if it is not, all of the assertions in the `quick_check` will likely fail even if (especially if) your gradients are implemented correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "4fecb738-819b-42d3-a417-8ef633c3864f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 8.052986383619253\n"
     ]
    }
   ],
   "source": [
    "def quick_check():\n",
    "    np.random.seed(159259)\n",
    "\n",
    "    w = np.random.random((5,))\n",
    "    c_pos = np.random.random((2, 5))\n",
    "    c_neg = np.random.random((4, 5))\n",
    "\n",
    "    eps = 1e-5\n",
    "\n",
    "    def approximate_gradient(func, vec, eps=1e-5):\n",
    "        est_grad = np.zeros(vec.shape)\n",
    "        for ind, el in np.ndenumerate(vec):\n",
    "            perturb = np.zeros(vec.shape)\n",
    "            perturb[ind] = eps\n",
    "            est_grad[ind] = (func(vec + perturb) - func(vec - perturb)) / (2 * eps)\n",
    "        return est_grad\n",
    "\n",
    "    print(\"loss:\", loss_fn(w, c_pos, c_neg))\n",
    "\n",
    "    assert np.allclose(w_grad(w, c_pos, c_neg), approximate_gradient(lambda x: loss_fn(x, c_pos, c_neg), w)), \"c_pos_grad is not correct for loss_fn\"\n",
    "    assert np.allclose(c_pos_grad(w, c_pos), approximate_gradient(lambda x: loss_fn(w, x, c_neg), c_pos)), \"c_pos_grad is not correct for loss_fn\"\n",
    "    assert np.allclose(c_neg_grad(w, c_neg), approximate_gradient(lambda x: loss_fn(w, c_pos, x), c_neg)), \"c_neg_grad is not correct for loss_fn\"\n",
    "\n",
    "quick_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd3d896-dace-4d50-b0c3-f065af19305b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Updating weights in the training loop\n",
    "\n",
    "The training loop for SGD consists of sampling one instance of the data (in our case, a target word and its positive and negative contexts), and calculating the partial derivatives of the loss.\n",
    "\n",
    "We then update the parameters using these partial derivatives, multiplying each gradient by the learning rate $\\eta$. When we perform gradient descent, we subtract the gradients from the weights in order to shift the weights in a direction that decreases the loss (locally, at least). Here are the updates we make:\n",
    "$$\n",
    "c_{\\text{pos}}^{t + 1} = c_{\\text{pos}}^{t} - \\eta \\frac{\\partial L}{\\partial {c}_{\\text{pos}}^t},\n",
    "$$\n",
    "$$\n",
    "c_{\\text{neg}}^{t + 1} = c_{\\text{neg}}^{t} - \\eta \\frac{\\partial L}{\\partial {c}_{\\text{neg}}^t}\n",
    ",$$\n",
    "$$\n",
    "w^{t + 1} = w^{t} - \\eta \\frac{\\partial L}{\\partial w^t}\n",
    ",$$\n",
    "where $t + 1$ is the next timestep in the stochastic gradient descent loop.\n",
    "\n",
    "**Note**: We print some diagnostic information, including the loss, to help you monitor the training. You should convince yourself that, though we calculate the loss and print it here to track our training, SGD doesn't actually require that we compute the loss as such; we really only need the gradients.\n",
    "\n",
    "You implement:\n",
    "- the section of the code where you calculate the gradients\n",
    "- the section of the code where you use the gradients to update the embedding\n",
    "\n",
    "You may want to read about [numpy indexing](https://numpy.org/doc/2.2/user/basics.indexing.html#), since the `.sample_contexts()` returns lists of indices; you might also want to look into [`np.subtract.at()`](https://numpy.org/doc/2.2/reference/generated/numpy.ufunc.at.html) (see the usage of `np.add.at()` in the starter code as another example).\n",
    "\n",
    "With a learning rate of 0.01, you should see some nearest neighbors start to make sense after about the loss drops under 60 or so. This took around 60K steps and 1m21s on our solution code; we recommend running for at least 10 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9ac359-0e50-4a96-8544-4bcf4ce2700a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "_Learning objectives_:\n",
    "> - Gain familiarity with training a classifier using stochastic gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "f6a656a1-706f-46e2-8ecf-0f684fd04e3c",
   "metadata": {
    "scrolled": true,
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/var/folders/n_/rj77m3n95lnctbnc2jkwbhw00000gn/T/ipykernel_17732/3844574401.py:24: RuntimeWarning: divide by zero encountered in matmul\n",
      "  vector_sim = ((vectors_norm)@(query_vector_norm)).ravel()\n",
      "/var/folders/n_/rj77m3n95lnctbnc2jkwbhw00000gn/T/ipykernel_17732/3844574401.py:24: RuntimeWarning: overflow encountered in matmul\n",
      "  vector_sim = ((vectors_norm)@(query_vector_norm)).ravel()\n",
      "/var/folders/n_/rj77m3n95lnctbnc2jkwbhw00000gn/T/ipykernel_17732/3844574401.py:24: RuntimeWarning: invalid value encountered in matmul\n",
      "  vector_sim = ((vectors_norm)@(query_vector_norm)).ravel()\n",
      "96it [00:00, 489.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: \n",
      "`he` was updated 0 times in target and 0 times in context\n",
      "['he', 'transponders', 'dusty', 'lse']\n",
      "`original` was updated 0 times in target and 0 times in context\n",
      "['original', 'quivering', 'saltire', 'bae']\n",
      "`january` was updated 0 times in target and 0 times in context\n",
      "['january', 'dailey', 'neutron', 'apogee']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10067it [00:18, 527.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 280.03156064652\n",
      "`he` was updated 62 times in target and 10611 times in context\n",
      "['he', 'transponders', 'dusty', 'lse']\n",
      "`original` was updated 0 times in target and 1099 times in context\n",
      "['original', 'quivering', 'saltire', 'bae']\n",
      "`january` was updated 5 times in target and 1686 times in context\n",
      "['january', 'dailey', 'neutron', 'apogee']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20057it [00:36, 487.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 280.03154296061166\n",
      "`he` was updated 115 times in target and 21116 times in context\n",
      "['he', 'transponders', 'dusty', 'lse']\n",
      "`original` was updated 3 times in target and 2208 times in context\n",
      "['original', 'quivering', 'saltire', 'bae']\n",
      "`january` was updated 9 times in target and 3225 times in context\n",
      "['january', 'dailey', 'neutron', 'apogee']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30082it [00:55, 522.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 280.0315253753248\n",
      "`he` was updated 167 times in target and 31796 times in context\n",
      "['he', 'transponders', 'dusty', 'lse']\n",
      "`original` was updated 4 times in target and 3275 times in context\n",
      "['original', 'quivering', 'saltire', 'bae']\n",
      "`january` was updated 18 times in target and 4856 times in context\n",
      "['january', 'dailey', 'neutron', 'apogee']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40078it [01:14, 512.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 280.03156137022694\n",
      "`he` was updated 240 times in target and 42343 times in context\n",
      "['he', 'transponders', 'dusty', 'lse']\n",
      "`original` was updated 7 times in target and 4240 times in context\n",
      "['original', 'quivering', 'saltire', 'bae']\n",
      "`january` was updated 25 times in target and 6522 times in context\n",
      "['january', 'dailey', 'neutron', 'apogee']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50054it [01:33, 502.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 280.0315563759385\n",
      "`he` was updated 295 times in target and 52852 times in context\n",
      "['he', 'transponders', 'dusty', 'lse']\n",
      "`original` was updated 10 times in target and 5207 times in context\n",
      "['original', 'quivering', 'saltire', 'bae']\n",
      "`january` was updated 29 times in target and 8107 times in context\n",
      "['january', 'dailey', 'neutron', 'apogee']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60090it [01:52, 509.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 280.0315331277815\n",
      "`he` was updated 358 times in target and 63553 times in context\n",
      "['he', 'transponders', 'dusty', 'lse']\n",
      "`original` was updated 11 times in target and 6261 times in context\n",
      "['original', 'quivering', 'saltire', 'bae']\n",
      "`january` was updated 32 times in target and 9738 times in context\n",
      "['january', 'dailey', 'neutron', 'apogee']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "70088it [02:11, 498.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 280.03154338647005\n",
      "`he` was updated 423 times in target and 74161 times in context\n",
      "['he', 'transponders', 'dusty', 'lse']\n",
      "`original` was updated 13 times in target and 7313 times in context\n",
      "['original', 'quivering', 'saltire', 'bae']\n",
      "`january` was updated 38 times in target and 11341 times in context\n",
      "['january', 'dailey', 'neutron', 'apogee']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80088it [02:30, 504.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 280.03152634301057\n",
      "`he` was updated 472 times in target and 84699 times in context\n",
      "['he', 'transponders', 'dusty', 'lse']\n",
      "`original` was updated 15 times in target and 8343 times in context\n",
      "['original', 'quivering', 'saltire', 'bae']\n",
      "`january` was updated 43 times in target and 12937 times in context\n",
      "['january', 'dailey', 'neutron', 'apogee']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "90068it [02:49, 497.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 280.0315451633482\n",
      "`he` was updated 524 times in target and 95044 times in context\n",
      "['he', 'transponders', 'dusty', 'lse']\n",
      "`original` was updated 22 times in target and 9393 times in context\n",
      "['original', 'quivering', 'saltire', 'bae']\n",
      "`january` was updated 49 times in target and 14583 times in context\n",
      "['january', 'dailey', 'neutron', 'apogee']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100067it [03:09, 498.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 280.0315518184826\n",
      "`he` was updated 581 times in target and 105553 times in context\n",
      "['he', 'transponders', 'dusty', 'lse']\n",
      "`original` was updated 26 times in target and 10475 times in context\n",
      "['original', 'quivering', 'saltire', 'bae']\n",
      "`january` was updated 52 times in target and 16161 times in context\n",
      "['january', 'dailey', 'neutron', 'apogee']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110092it [03:28, 494.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 280.0315336327573\n",
      "`he` was updated 617 times in target and 116189 times in context\n",
      "['he', 'transponders', 'dusty', 'lse']\n",
      "`original` was updated 31 times in target and 11503 times in context\n",
      "['original', 'quivering', 'saltire', 'bae']\n",
      "`january` was updated 59 times in target and 17812 times in context\n",
      "['january', 'dailey', 'neutron', 'apogee']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120042it [03:47, 486.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 280.03155113914374\n",
      "`he` was updated 682 times in target and 126782 times in context\n",
      "['he', 'transponders', 'dusty', 'lse']\n",
      "`original` was updated 34 times in target and 12570 times in context\n",
      "['original', 'quivering', 'saltire', 'bae']\n",
      "`january` was updated 61 times in target and 19484 times in context\n",
      "['january', 'dailey', 'neutron', 'apogee']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "130087it [04:07, 484.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 280.0315524725839\n",
      "`he` was updated 747 times in target and 137361 times in context\n",
      "['he', 'transponders', 'dusty', 'lse']\n",
      "`original` was updated 36 times in target and 13607 times in context\n",
      "['original', 'quivering', 'saltire', 'bae']\n",
      "`january` was updated 70 times in target and 21095 times in context\n",
      "['january', 'dailey', 'neutron', 'apogee']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "140084it [04:27, 483.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 280.0315494403289\n",
      "`he` was updated 805 times in target and 147898 times in context\n",
      "['he', 'transponders', 'dusty', 'lse']\n",
      "`original` was updated 38 times in target and 14703 times in context\n",
      "['original', 'quivering', 'saltire', 'bae']\n",
      "`january` was updated 71 times in target and 22715 times in context\n",
      "['january', 'dailey', 'neutron', 'apogee']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150076it [04:46, 488.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 280.0315523743929\n",
      "`he` was updated 852 times in target and 158632 times in context\n",
      "['he', 'transponders', 'dusty', 'lse']\n",
      "`original` was updated 40 times in target and 15756 times in context\n",
      "['original', 'quivering', 'saltire', 'bae']\n",
      "`january` was updated 79 times in target and 24340 times in context\n",
      "['january', 'dailey', 'neutron', 'apogee']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160087it [05:06, 486.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 280.0315516375063\n",
      "`he` was updated 910 times in target and 169165 times in context\n",
      "['he', 'transponders', 'dusty', 'lse']\n",
      "`original` was updated 42 times in target and 16795 times in context\n",
      "['original', 'quivering', 'saltire', 'bae']\n",
      "`january` was updated 87 times in target and 26011 times in context\n",
      "['january', 'dailey', 'neutron', 'apogee']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "168556it [05:23, 521.37it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[454], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m             np\u001b[38;5;241m.\u001b[39madd\u001b[38;5;241m.\u001b[39mat(num_context_updates, neg, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     47\u001b[0m w2v_model \u001b[38;5;241m=\u001b[39m Word2Vec(dataloader)\n\u001b[0;32m---> 48\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw2v_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[454], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[1;32m     14\u001b[0m     losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (target, pos, neg) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(dataloader\u001b[38;5;241m.\u001b[39msample_contexts(window_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, sample_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m))):\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10_000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     18\u001b[0m             \u001b[38;5;66;03m# Print diagnostic info every 10_000 steps.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28msum\u001b[39m(losses) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(losses) \u001b[38;5;28;01mif\u001b[39;00m losses \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/sklearn-env/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[121], line 67\u001b[0m, in \u001b[0;36mFileDataLoader.sample_contexts\u001b[0;34m(self, window_size, sample_k)\u001b[0m\n\u001b[1;32m     65\u001b[0m target_word_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab2idx[doc[word_idx]] \u001b[38;5;28;01mif\u001b[39;00m doc[word_idx] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab2idx \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab2idx[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[UNK]\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# sample positive words from the window\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m positive_word_idxs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab2idx[word] \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab2idx \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab2idx[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[UNK]\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m doc[word_idx \u001b[38;5;241m-\u001b[39m window_size:word_idx] \u001b[38;5;241m+\u001b[39m doc[word_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m:word_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m window_size]\n\u001b[1;32m     69\u001b[0m     \n\u001b[1;32m     70\u001b[0m ])\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# sample len(positive_word_idxs) * sample_k number of negative words\u001b[39;00m\n\u001b[1;32m     72\u001b[0m negative_word_idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnegative_sample(target_word_idx, sample_k \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(positive_word_idxs))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "def train(model, dataloader):\n",
    "\n",
    "    num_target_updates = np.zeros((model.target_embs.shape[0],))\n",
    "    num_context_updates = np.zeros((model.context_embs.shape[0],))\n",
    "\n",
    "    def print_diagnostic(word):\n",
    "        print(f\"`{word}` was updated {int(num_target_updates[dataloader.vocab2idx[word]])} times in target and {int(num_context_updates[dataloader.vocab2idx[word]])} times in context\")\n",
    "        model.print_nearest_neighbors(word, 4)\n",
    "\n",
    "    for i in range(NUM_EPOCHS):\n",
    "        losses = []\n",
    "        for i, (target, pos, neg) in enumerate(tqdm(dataloader.sample_contexts(window_size=2, sample_k=100))):\n",
    "\n",
    "            if i % 10_000 == 0:\n",
    "                # Print diagnostic info every 10_000 steps.\n",
    "                print(\"avg loss:\", sum(losses) / len(losses) if losses else \"\")\n",
    "                losses = []\n",
    "                print_diagnostic(\"he\")\n",
    "                print_diagnostic(\"original\")\n",
    "                print_diagnostic(\"january\")\n",
    "\n",
    "            # Get the vectors from the model\n",
    "            w = model.target_embs[target]\n",
    "            c_pos = model.context_embs[pos]\n",
    "            c_neg = model.context_embs[neg]\n",
    "\n",
    "            # Calculate and store the loss\n",
    "            losses.append(loss_fn(w, c_pos, c_neg))\n",
    "\n",
    "            # TODO: Calculate the gradients and implement the gradient update.\n",
    "            w_new = w-LEARNING_RATE*w_grad(w, c_pos, c_neg)\n",
    "            c_neg_new = c_neg-LEARNING_RATE*c_neg_grad(w, c_neg)\n",
    "            c_pos_new = c_pos-LEARNING_RATE*c_pos_grad(w, c_pos)\n",
    "\n",
    "            w = w_new\n",
    "            c_neg = c_neg_new\n",
    "            c_pos = c_pos_new\n",
    "            \n",
    "            # Tally up how many times each word has been seen, just for fun.\n",
    "            np.add.at(num_target_updates, target, 1)\n",
    "            np.add.at(num_context_updates, pos, 1)\n",
    "            np.add.at(num_context_updates, neg, 1)\n",
    "\n",
    "w2v_model = Word2Vec(dataloader)\n",
    "train(w2v_model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d25f981-30ac-4df4-99af-70c78619dcd7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Once you are satisfied with the training (you can stop it whenever you want), experiment with printing out some nearest neighbors. Do these align with your expectations? Do any surprise you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "a36e02da-0c43-48b5-8105-8c6c72cab768",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "7b5caadc-5b40-4119-aa7b-075f12ab2055",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['paris', 'settled', 'suppresses', 'wan']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n_/rj77m3n95lnctbnc2jkwbhw00000gn/T/ipykernel_17732/3844574401.py:24: RuntimeWarning: divide by zero encountered in matmul\n",
      "  vector_sim = ((vectors_norm)@(query_vector_norm)).ravel()\n",
      "/var/folders/n_/rj77m3n95lnctbnc2jkwbhw00000gn/T/ipykernel_17732/3844574401.py:24: RuntimeWarning: overflow encountered in matmul\n",
      "  vector_sim = ((vectors_norm)@(query_vector_norm)).ravel()\n",
      "/var/folders/n_/rj77m3n95lnctbnc2jkwbhw00000gn/T/ipykernel_17732/3844574401.py:24: RuntimeWarning: invalid value encountered in matmul\n",
      "  vector_sim = ((vectors_norm)@(query_vector_norm)).ravel()\n"
     ]
    }
   ],
   "source": [
    "model.print_nearest_neighbors(\"paris\", 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ef471a-ff16-4d20-923f-7a5aea813a16",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c000e0b3-1ef4-4ba9-b3b5-cc4690a92ca5",
   "metadata": {},
   "source": [
    "Congratulations on finishing HW1!\n",
    "\n",
    "Please ensure that you submit a PDF of this notebook onto [Gradescope](https://www.gradescope.com/courses/1238346) before February 3 at 11:59pm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9589c7fa-68cf-4bee-88b7-78bf92c2dc89",
   "metadata": {},
   "source": [
    "You can run the cell below to generate a PDF if you are using Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b252b823-44c7-4d4a-ae50-03cabaac3913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPORT_EXCLUDE#\n",
    "\n",
    "#@markdown This is a helper function to generate a PDF in Colab.\n",
    "#@markdown If you are using Jupyter notebook, you can do `File > Save and Export Notebook as HTML`, then save the resulting HTML file as a PDF.\n",
    "#@markdown Alternatively, in Juypter notebook, you might try `File > Save and Export Notebook as PDF`, but just make sure you already have `pandoc` installed.\n",
    "\n",
    "def colab_export_pdf():\n",
    "    # Modified from: https://medium.com/@jonathanagustin/convert-colab-notebook-to-pdf-0ccd8f847dd6\n",
    "    try:\n",
    "        import google.colab\n",
    "        IN_COLAB = True\n",
    "    except:\n",
    "        IN_COLAB = False\n",
    "        print(\"This cell only works in Google Colab!\")\n",
    "        print(\"If you are running locally, click File > Export as HTML. Then open the HTML file and save it as a PDF.\")\n",
    "\n",
    "    if IN_COLAB:\n",
    "        print(\"Generating PDF. This may take a few seconds.\")\n",
    "        import os, datetime, json, locale, pathlib, urllib, requests, werkzeug, nbformat, google, yaml, warnings\n",
    "        locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n",
    "        NAME = pathlib.Path(werkzeug.utils.secure_filename(urllib.parse.unquote(requests.get(f\"http://{os.environ['COLAB_JUPYTER_IP']}:{os.environ['KMP_TARGET_PORT']}/api/sessions\").json()[0][\"name\"])))\n",
    "        TEMP = pathlib.Path(\"/content/pdfs\") / f\"{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}_{NAME.stem}\"; TEMP.mkdir(parents=True, exist_ok=True)\n",
    "        NB = [cell for cell in nbformat.reads(json.dumps(google.colab._message.blocking_request(\"get_ipynb\", timeout_sec=30)[\"ipynb\"]), as_version=4).cells if \"--Colab2PDF\" not in cell.source]\n",
    "        warnings.filterwarnings('ignore', category=nbformat.validator.MissingIDFieldWarning)\n",
    "        with (TEMP / f\"{NAME.stem}.ipynb\").open(\"w\", encoding=\"utf-8\") as nb_copy: nbformat.write(nbformat.v4.new_notebook(cells=NB or [nbformat.v4.new_code_cell(\"#\")]), nb_copy)\n",
    "        if not pathlib.Path(\"/usr/local/bin/quarto\").exists():\n",
    "            !wget -q \"https://quarto.org/download/latest/quarto-linux-amd64.deb\" -P {TEMP} && dpkg -i {TEMP}/quarto-linux-amd64.deb > /dev/null && quarto install tinytex --update-path --quiet\n",
    "        with (TEMP / \"config.yml\").open(\"w\", encoding=\"utf-8\") as file: yaml.dump({'include-in-header': [{\"text\": r\"\\usepackage{fvextra}\\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaksymbolleft={},showspaces=false,showtabs=false,breaklines,breakanywhere,commandchars=\\\\\\{\\}}\"}],'include-before-body': [{\"text\": r\"\\DefineVerbatimEnvironment{verbatim}{Verbatim}{breaksymbolleft={},showspaces=false,showtabs=false,breaklines}\"}]}, file)\n",
    "        !quarto render {TEMP}/{NAME.stem}.ipynb --metadata-file={TEMP}/config.yml --to pdf -M latex-auto-install -M margin-top=1in -M margin-bottom=1in -M margin-left=1in -M margin-right=1in --quiet\n",
    "        google.colab.files.download(str(TEMP / f\"{NAME.stem}.pdf\"))\n",
    "\n",
    "colab_export_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5682dd69-85ff-4f21-84e6-64ee3146cdb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Sci-Kit Learn Env",
   "language": "python",
   "name": "sklearn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
