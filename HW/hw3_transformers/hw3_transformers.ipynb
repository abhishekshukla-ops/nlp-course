{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "689cbcbc-f951-4fd1-95ee-ff2bac2f14b7",
      "metadata": {
        "id": "689cbcbc-f951-4fd1-95ee-ff2bac2f14b7"
      },
      "source": [
        "# INFO 159/259"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "484bdfab-9192-4359-a390-63f40c70029a",
      "metadata": {
        "id": "484bdfab-9192-4359-a390-63f40c70029a"
      },
      "source": [
        "# <center> Homework 3: Transformers and Masked Language Models </center>\n",
        "<center> Due: <b>Monday</b>, February 23, 2026 @ 11:59pm </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9caee50a-3b0b-4286-aea5-9ffd7e4aaf91",
      "metadata": {
        "id": "9caee50a-3b0b-4286-aea5-9ffd7e4aaf91"
      },
      "source": [
        "In this homework, you will experiment with a BERT-style bi-directional encoder transformer model that has been pretrained on the masked language modeling task.\n",
        "\n",
        "In the first part, you will extract the _contextual_ embedding representations of words in the context of their sentences.\n",
        "\n",
        "In the second part, you will explore how the embedding representation changes at different layers of the model.\n",
        "\n",
        "\n",
        "Learning objectives:\n",
        "- Understand the masked language modeling objective\n",
        "- Be able to explain the difference between contextual and static representations\n",
        "- Understand the transformers architecture\n",
        "\n",
        "During the course of this project, you may need to inspect intermediate outputs to understand how they are structured (e.g., what exactly is in the output of the BERT model?) Feel free to create extra cells to do this investigation.\n",
        "\n",
        "You should also be using a GPU instance on Google Colab to run your code. You can enable a GPU kernel by clicking `Runtime > Change runtime type` and selecting `T4 GPU`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1a2cbc8-3234-4658-809c-f871f53b47fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "d1a2cbc8-3234-4658-809c-f871f53b47fd",
        "outputId": "0559c063-8864-4442-a38f-5ebcce67ab36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2026-02-22 22:01:51--  https://github.com/dbamman/nlp-course/raw/refs/heads/main/HW/data/promotion.n.xml\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/dbamman/nlp-course/refs/heads/main/HW/data/promotion.n.xml [following]\n",
            "--2026-02-22 22:01:52--  https://raw.githubusercontent.com/dbamman/nlp-course/refs/heads/main/HW/data/promotion.n.xml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2843173 (2.7M) [text/plain]\n",
            "Saving to: ‘promotion.n.xml’\n",
            "\n",
            "promotion.n.xml     100%[===================>]   2.71M  12.0MB/s    in 0.2s    \n",
            "\n",
            "2026-02-22 22:01:52 (12.0 MB/s) - ‘promotion.n.xml’ saved [2843173/2843173]\n",
            "\n",
            "--2026-02-22 22:01:52--  https://github.com/dbamman/nlp-course/raw/refs/heads/main/HW/hw3_transformers/hw3_utils.py\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/dbamman/nlp-course/refs/heads/main/HW/hw3_transformers/hw3_utils.py [following]\n",
            "--2026-02-22 22:01:53--  https://raw.githubusercontent.com/dbamman/nlp-course/refs/heads/main/HW/hw3_transformers/hw3_utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 525 [text/plain]\n",
            "Saving to: ‘hw3_utils.py’\n",
            "\n",
            "hw3_utils.py        100%[===================>]     525  --.-KB/s    in 0s      \n",
            "\n",
            "2026-02-22 22:01:53 (44.3 MB/s) - ‘hw3_utils.py’ saved [525/525]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/dbamman/nlp-course/raw/refs/heads/main/HW/data/promotion.n.xml -O promotion.n.xml\n",
        "!wget https://github.com/dbamman/nlp-course/raw/refs/heads/main/HW/hw3_transformers/hw3_utils.py -O hw3_utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c32a029-b45f-4f11-b6a4-b329f3ce8f23",
      "metadata": {
        "id": "1c32a029-b45f-4f11-b6a4-b329f3ce8f23"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertModel, AutoTokenizer\n",
        "\n",
        "from hw3_utils import load_data_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26ee5813-052b-4763-9828-de92662fa4e4",
      "metadata": {
        "id": "26ee5813-052b-4763-9828-de92662fa4e4"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bcf29d1-6e64-498a-8145-a225400d35bc",
      "metadata": {
        "id": "9bcf29d1-6e64-498a-8145-a225400d35bc"
      },
      "outputs": [],
      "source": [
        "items = load_data_file(\"promotion.n.xml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f24cf5a-1325-4b21-9b93-1eeb18060f2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444,
          "referenced_widgets": [
            "272a0fc883be402f8e9d9679b6707eaa",
            "aaff426417cd410a83591660b6ea7ed5",
            "570af17f569946beae25aac7be7b5753",
            "15e23348500c445dbcd745f334490f0e",
            "30f34eb106294ba48d6ba5f7edc721d9",
            "8e1622c0603a43cf922b1de4abba80ea",
            "536a7979abed4203ba0d4ec5d8d8d708",
            "71c9fcb049cf44deb793ac80267904bf",
            "b011cb4cc4934131882b583cdc44ec57",
            "bb6b939777484b06800ab17920693bec",
            "dbfccb4a0f654330b36262c7b58a508b"
          ]
        },
        "id": "9f24cf5a-1325-4b21-9b93-1eeb18060f2d",
        "outputId": "1cd9084e-6b75-4570-d0f3-e748970fcaee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "272a0fc883be402f8e9d9679b6707eaa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "BertModel LOAD REPORT from: bert-base-uncased\n",
            "Key                                        | Status     |  | \n",
            "-------------------------------------------+------------+--+-\n",
            "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |  | \n",
            "cls.seq_relationship.weight                | UNEXPECTED |  | \n",
            "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |  | \n",
            "cls.predictions.bias                       | UNEXPECTED |  | \n",
            "cls.predictions.transform.dense.weight     | UNEXPECTED |  | \n",
            "cls.predictions.transform.dense.bias       | UNEXPECTED |  | \n",
            "cls.seq_relationship.bias                  | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        }
      ],
      "source": [
        "# You may get a warning in the LOAD REPORT. This is ok!\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bde5e12-4a21-4682-8d8f-34b29cb9d6b3",
      "metadata": {
        "id": "2bde5e12-4a21-4682-8d8f-34b29cb9d6b3"
      },
      "source": [
        "## Tokenizing text\n",
        "\n",
        "We will begin by implementing the `tokenize` and `collate` functions. These two functions, together, will prepare your data to be fed into the BertModel (see the [huggingface documentation here](https://huggingface.co/docs/transformers/main/en/model_doc/bert#transformers.BertModel.forward).)\n",
        "\n",
        "The `tokenize` function converts a sentence into the token IDs that are recognizable to the model; it also generates an attention mask. It takes an input batch (a dictionary of lists) and returns an output batch (also a dictionary of lists).\n",
        "\n",
        "```\n",
        "Input:\n",
        "{\n",
        "    \"word\": list[str]\n",
        "    \"sentence\": list[list[str]]\n",
        "}\n",
        "\n",
        "Output:\n",
        "{\n",
        "    # These are generated by calling tokenizer()\n",
        "    \"input_ids\": list[list[int]]\n",
        "    \"token_type_ids\": list[list[int]]\n",
        "    \"attention_mask\": list[list[int]]\n",
        "\n",
        "    # Write the code to find the token indices\n",
        "    \"token_indices\": list[int]\n",
        "}\n",
        "```\n",
        "\n",
        "You will want to store the index of the first subword token that corresponds to the target word in `batch[\"word\"]`. Here is an example:\n",
        "\n",
        "```\n",
        "Text:            I    said    hello    world    .\n",
        "\n",
        "Tokens: [CLS]    I    said    hello    world    .    [SEP]\n",
        "IDs:     101   1045   2056    7592     2088    1012   102\n",
        "Index:    0      1     2        3        4      5      6\n",
        "                                ^\n",
        "```\n",
        "The token `hello` has token index 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UYtulwlh-IXW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYtulwlh-IXW",
        "outputId": "ae3e7018-282a-42d7-afed-d9a6f3af8e9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [[101, 2054, 2003, 2026, 2171, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1]]}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer([\"What is my name\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z8YrfOyDEQ0-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8YrfOyDEQ0-",
        "outputId": "2fc8da3f-6e1a-47b2-a956-dce8bd4cc81a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "a = [\"1\", \"2\", \"2\"]\n",
        "a.index(\"2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e1f5d27-97f7-4a36-a5af-670e6bb3f7b3",
      "metadata": {
        "id": "6e1f5d27-97f7-4a36-a5af-670e6bb3f7b3"
      },
      "outputs": [],
      "source": [
        "def tokenize(batch):\n",
        "    # TODO: Implement me!\n",
        "    # print(batch)\n",
        "    tokenizer_output = tokenizer(batch[\"sentence\"], is_split_into_words=True)\n",
        "\n",
        "    output = dict()\n",
        "\n",
        "    output['input_ids'] = tokenizer_output['input_ids']\n",
        "    output['token_type_ids'] = tokenizer_output['token_type_ids']\n",
        "    output['attention_mask'] = tokenizer_output['attention_mask']\n",
        "\n",
        "    token_indices = []\n",
        "\n",
        "    for i, word_ in enumerate(batch[\"word\"]):\n",
        "      word_index = batch['sentence'][i].index(word_)\n",
        "      token_indices.append(word_index+1)\n",
        "\n",
        "    output['token_indices'] = token_indices\n",
        "\n",
        "\n",
        "    assert not any(x is None for x in output[\"token_indices\"]), \"Target token not found in sentence!\"\n",
        "    assert len(token_indices) == len(batch[\"sentence\"]), \"Token indices is the wrong length!\"\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fOwBCvqo6qUT",
      "metadata": {
        "id": "fOwBCvqo6qUT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "f010bd65-a167-4417-bfb2-c0ae4fbd8918",
      "metadata": {
        "id": "f010bd65-a167-4417-bfb2-c0ae4fbd8918"
      },
      "source": [
        "**Quick check**: This should be the output of the below cell. Your decoded token should also match the target word.\n",
        "```\n",
        "{'input_ids': [[101, 1045, 2056, 7592, 2088, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1]], 'token_indices': [4]}\n",
        "\n",
        "Selected token ID:  2088\n",
        "Decoded token:  world\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98dfecf5-fd95-4714-87dd-ba6a6ef16d0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98dfecf5-fd95-4714-87dd-ba6a6ef16d0a",
        "outputId": "7069f93e-b4f0-4a26-b106-d999fd624f82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [[101, 1045, 2056, 7592, 2088, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1]], 'token_indices': [4]}\n",
            "\n",
            "Selected token ID:  2088\n",
            "Decoded token:  world\n"
          ]
        }
      ],
      "source": [
        "def _():\n",
        "    tokens = tokenize({\n",
        "        \"word\": [\"world\"],\n",
        "        \"sentence\": [[\"I\", \"said\", \"hello\", \"world\", \".\"]]\n",
        "    })\n",
        "    print(tokens)\n",
        "    print()\n",
        "\n",
        "    print(\"Selected token ID: \", tokens[\"input_ids\"][0][tokens[\"token_indices\"][0]])\n",
        "    print(\"Decoded token: \", tokenizer.decode(tokens[\"input_ids\"][0][tokens[\"token_indices\"][0]]))\n",
        "\n",
        "_()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f8a151f-310f-4d6b-ac0f-375217b044b3",
      "metadata": {
        "id": "1f8a151f-310f-4d6b-ac0f-375217b044b3"
      },
      "source": [
        "The `collate` function converts a list of rows into a batched tensor that the model can process in parallel. It takes a list of rows (a list of dicts) and returns a dictionary of tensors that can be fed into the model. It should use the `tokenize` function you implemented.\n",
        "\n",
        "```\n",
        "Input:\n",
        "[{\"word\": str, \"sentence\": str}, ...]\n",
        "\n",
        "Output:\n",
        "{\n",
        "    \"input_ids\": torch.tensor\n",
        "    \"token_type_ids\": torch.tensor\n",
        "    \"attention_mask\": torch.tensor\n",
        "    \"token_indices\": torch.tensor\n",
        "}\n",
        "```\n",
        "\n",
        "Since sentences might be different lengths, you will want to `pad` the sequences before converting to torch tensors. You might want to look into `torch.nn.utils.rnn.pad_sequence`.\n",
        "\n",
        "Each of `input_ids`, `token_type_ids`, and `attention_mask` should have shape `(B, L)`, where `B` is the batch size and `L` is the maximum sequence length in the batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4425e1f6-ae0b-4a21-9053-caf769264755",
      "metadata": {
        "id": "4425e1f6-ae0b-4a21-9053-caf769264755"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate(items, device=\"cpu\"):\n",
        "    item_batch = {key: [item[key] for item in items] for key in items[0].keys()}\n",
        "    tokenized_items = tokenize(item_batch)\n",
        "    # print(tokenized_items)\n",
        "\n",
        "    outputs = dict()\n",
        "    for key in ['input_ids', 'token_type_ids', 'attention_mask']:\n",
        "      outputs[key] = pad_sequence([torch.tensor(x) for x in tokenized_items[key]]).T\n",
        "\n",
        "    outputs['token_indices'] = torch.tensor(tokenized_items['token_indices'])\n",
        "\n",
        "    return outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2c65237-5e1a-4401-923b-3bb7f31c6d80",
      "metadata": {
        "id": "f2c65237-5e1a-4401-923b-3bb7f31c6d80"
      },
      "source": [
        "**Quick check**: This should be the output of the following cell.\n",
        "\n",
        "```\n",
        "{'input_ids': tensor(\n",
        "    [[  101, 18558, 18914,  2003,  2019, 17953,  2361,  2607,  1012,   102],\n",
        "     [  101,  1045,  2293, 17953,  2361,   999,   102,     0,     0,     0],\n",
        "     [  101,  2054,  2515, 17953,  2361,  3233,  2005,  1029,  1029,   102]]),\n",
        " 'token_type_ids': tensor(\n",
        "    [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
        " 'attention_mask': tensor(\n",
        "    [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "     [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
        "     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
        " 'token_indices': tensor([5, 3, 3])}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19b940eb-9519-482e-a4d7-8ff087aaeb91",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19b940eb-9519-482e-a4d7-8ff087aaeb91",
        "outputId": "3a9acbe4-7d69-4d22-9aba-9a179b20a3c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101, 18558, 18914,  2003,  2019, 17953,  2361,  2607,  1012,   102],\n",
              "         [  101,  1045,  2293, 17953,  2361,   999,   102,     0,     0,     0],\n",
              "         [  101,  2054,  2515, 17953,  2361,  3233,  2005,  1029,  1029,   102]]),\n",
              " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
              " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
              "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
              " 'token_indices': tensor([5, 3, 3])}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def _():\n",
        "    return collate([\n",
        "        dict(word=\"NLP\", sentence=\"INFO 159 is an NLP course .\".split()),\n",
        "        dict(word=\"NLP\", sentence=\"I love NLP !\".split()),\n",
        "        dict(word=\"NLP\", sentence=\"What does NLP stand for ? ?\".split()),\n",
        "    ])\n",
        "_()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aRr1yWW2cRn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aRr1yWW2cRn",
        "outputId": "3f74af59-ee6f-4b47-f4a7-7e7fd25a9989"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[101, 17953, 2361, 102]]\n",
            "nl\n",
            "##p\n",
            "nlp\n",
            "[CLS] [SEP] [PAD]\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.encode(['NLP']))\n",
        "print(tokenizer.decode([17953]))\n",
        "print(tokenizer.decode([2361]))\n",
        "print(tokenizer.decode([17953, 2361]))\n",
        "print(tokenizer.decode([101, 102, 0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f24119d-9a17-455c-aeb9-650e900eedf6",
      "metadata": {
        "id": "1f24119d-9a17-455c-aeb9-650e900eedf6"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Answer these questions.** (You may execute arbitrary code if necessary.)\n",
        "1. Why are there more `input_ids` in each sequence compared to the number of (space-delimited) tokens?\n",
        "2. List all of the extra (special) tokens that get added to the input. Write the decoded tokens (not the integer input IDs)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d74fec1b",
      "metadata": {
        "id": "d74fec1b"
      },
      "source": [
        "1. This is because of the special tokens added in the end and beginning of the sequences and also complex words are broken down into sub-words. Example: nlp broken down to nl and ##p (17953, 2361)\n",
        "2. They are [CLS], [SEP] and [PAD]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f848a11-1f86-47c5-be13-c2e45a3cf80f",
      "metadata": {
        "id": "0f848a11-1f86-47c5-be13-c2e45a3cf80f"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "## Extracting contextual embedding\n",
        "\n",
        "Here, you will implement `get_token_embedding` to get the contextual embedding. For the $i$th sentence in the batch, you want to extract the embedding representation for the $i$th token index in `token_indices` at the specified layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_0mGeKZ-5dAF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0mGeKZ-5dAF",
        "outputId": "3f40fced-ae3a-4b25-8981-be0e78605202"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 2, 768])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.embeddings(torch.tensor([[101, 102]])).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63c71908-5af4-478c-a05c-52e74a3a8067",
      "metadata": {
        "id": "63c71908-5af4-478c-a05c-52e74a3a8067"
      },
      "outputs": [],
      "source": [
        "from pygments import token\n",
        "def get_token_embedding(model_output, token_indices, layer=-1):\n",
        "    # batch_reps should have shape (B, D)\n",
        "    # where B is the batch size and D is the dimension of the hidden state\n",
        "    # (for BERT, D = 768)\n",
        "\n",
        "\n",
        "    embedding_dim = 768\n",
        "    batch_reps = model_output['hidden_states'][layer][0][token_indices]\n",
        "\n",
        "    return batch_reps.detach().cpu()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1HR_BqkV9eMY",
      "metadata": {
        "id": "1HR_BqkV9eMY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ie4fFk3G9dsq",
      "metadata": {
        "id": "ie4fFk3G9dsq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "9712aaa3-c4d6-4132-bb54-4fb72ad3e8a7",
      "metadata": {
        "id": "9712aaa3-c4d6-4132-bb54-4fb72ad3e8a7"
      },
      "source": [
        "We implement the inference code for you, but read through it and make sure you understand what is going on! Calling `model(...)` calls the `.forward()` method of the model as well as the necessary pre- and post-processing steps (see the [HF documentation](https://huggingface.co/docs/transformers/main/en/model_doc/bert#transformers.BertModel.forward) for the BertModel, and the note at the bottom about overriding the `__call__` method).\n",
        "\n",
        "Setting up the code this way (with `iter_outputs` serving as a generator that yields model outputs) lets us easily iterate through model outputs and apply arbitrary functions to them (like our `get_token_embedding` function)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "341b326a-d7cb-421a-8eac-e191f677b28d",
      "metadata": {
        "id": "341b326a-d7cb-421a-8eac-e191f677b28d"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "def iter_outputs(data, model, batch_size=128):\n",
        "    model.eval()  # setting eval mode disables dropout (and other stuff)\n",
        "    model.to(device)  # we put the model on the GPU\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        data,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate  # we pass in our collate function here\n",
        "    )\n",
        "\n",
        "    # disable gradient calculation and storage for efficiency,\n",
        "    # since we aren't backpropagating\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            output = model(\n",
        "                input_ids=batch[\"input_ids\"].to(device),\n",
        "                attention_mask=batch[\"attention_mask\"].to(device),\n",
        "                token_type_ids=batch[\"token_type_ids\"].to(device),\n",
        "                # by default, this only returns the last layer hidden states\n",
        "                # we want the flexibility to look at other layers, so we set\n",
        "                # `output_hidden_states=True`\n",
        "                output_hidden_states=True\n",
        "            )\n",
        "            yield batch, output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a224f27-c081-4ead-9c72-b192268ad4b3",
      "metadata": {
        "id": "5a224f27-c081-4ead-9c72-b192268ad4b3"
      },
      "source": [
        "**Quick check**: you should get the following output\n",
        "```\n",
        "tensor([[ 0.5100, -1.1154,  0.6605,  ..., -0.2650,  1.5101,  0.5656],\n",
        "        [ 0.9360, -1.9150,  0.8224,  ...,  0.1231,  0.9546, -0.1965]])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ad3588e-057f-4281-8432-c620fc7e3be5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ad3588e-057f-4281-8432-c620fc7e3be5",
        "outputId": "8de0facd-9879-45ca-8137-c0509522d257"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  8.50it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.5100, -1.1154,  0.6605,  ..., -0.2650,  1.5101,  0.5655],\n",
              "        [ 0.9360, -1.9150,  0.8224,  ...,  0.1231,  0.9546, -0.1965]])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def _():\n",
        "    embeddings = []\n",
        "    for batch, batch_output in iter_outputs([dict(word=\"NLP\", sentence=\"INFO 159 is an NLP course .\".split())], model):\n",
        "        embeddings.append(get_token_embedding(batch_output, batch[\"token_indices\"]))\n",
        "        embeddings.append(get_token_embedding(batch_output, batch[\"token_indices\"], layer=4))\n",
        "    embeddings = torch.concat(embeddings, dim = 0)\n",
        "    return embeddings\n",
        "\n",
        "_()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a345261-ea7d-4f86-9586-685e8ae16b36",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a345261-ea7d-4f86-9586-685e8ae16b36",
        "outputId": "3af865ac-ea63-4c38-ae7b-8403cc9ca2ad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 34/34 [42:18<00:00, 74.67s/it]\n"
          ]
        }
      ],
      "source": [
        "def get_all_embeddings():\n",
        "    embeddings = []\n",
        "    for batch, batch_output in iter_outputs(items, model):\n",
        "        embeddings.append(get_token_embedding(batch_output, batch[\"token_indices\"]))\n",
        "\n",
        "    return torch.concat(embeddings, dim=0)\n",
        "\n",
        "embeddings = get_all_embeddings()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccb88c4a-e363-4078-b67a-7e3d15d510c9",
      "metadata": {
        "id": "ccb88c4a-e363-4078-b67a-7e3d15d510c9"
      },
      "source": [
        "## Exploring contextual embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26948cdc-f23e-427b-ae74-2d9d46620cc1",
      "metadata": {
        "id": "26948cdc-f23e-427b-ae74-2d9d46620cc1"
      },
      "source": [
        "With contextual embeddings, the representations change depending on the context; let's see that in action by looking at the sentences where the target word embeddings have the greatest similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2744ce21-572a-4c83-8866-07566bca1cb0",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2744ce21-572a-4c83-8866-07566bca1cb0"
      },
      "outputs": [],
      "source": [
        "def nearest_neighbors(vec, matrix, k=10):\n",
        "    cos_sim = ((vec @ matrix.T) / (vec.norm() * matrix.T.norm(dim=0, keepdim=True))).squeeze()\n",
        "    inds = torch.argsort(-cos_sim)[:k]\n",
        "    return inds, cos_sim[inds]\n",
        "\n",
        "def show_nearest_neighbor_sentences(index, embeddings):\n",
        "    print(f\"QUERY (target word={items[index]['word']}):\")\n",
        "    print(\" \".join(items[index][\"sentence\"]))\n",
        "\n",
        "    print(\"NEIGHBORS:\")\n",
        "    inds, _ = nearest_neighbors(embeddings[index], embeddings, k=5)\n",
        "    for ind in inds:\n",
        "        print(\"-\", \" \".join(items[ind][\"sentence\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd99a961-ecb9-479f-a289-f3b26c9a60e3",
      "metadata": {
        "id": "bd99a961-ecb9-479f-a289-f3b26c9a60e3"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Answer the following question**\n",
        "1. Identify at least two different contexts in which the target word `promotion` appears. Include the query sentence, one nearest neighbor (not the query), and the ID of the query sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da3f4dd2",
      "metadata": {
        "id": "da3f4dd2"
      },
      "source": [
        "Query ID: 436\n",
        "\n",
        "Query Sentence: \"Career ladders establish a pathway for career advancement leading to the full performance level of the position. After initial competition to enter the career ladder , successive **promotion** is dependent upon ( 1 ) the employee meeting legal and regulatory requirements ( time-in-grade restrictions ) , ( 2 ) the employee 's performance , and ( 3 ) the need for and availability of higher level work within the organization. Employees are not guaranteed promotion once selected for a position within an established career ladder. However , managers are encouraged to foster a work environment that affords individuals assigned to career ladders an equal opportunity to demonstrate their ability to perform at the full performance level .\"\n",
        "\n",
        "Nearest Neighbor: \"Within the College , candidates for **promotion** and/or tenure are evaluated by their District Department Head or Program Department Head , Promotion and Tenure Committee , Associate Dean and Associate Director , and Dean and Chief Administrative Officer. At all levels of this evaluation , judgments must be made based on an individual 's responsibilities and performance. These judgments should recognize that each faculty member has a unique responsibility within the University. Likewise , the candidate must be aware that advancement through the academic ranks requires not only excellence in an academic discipline , but also evidence of developing the professional stature and maturity of view expected of those in the professorial ranks. Candidates for promotion and/or tenure are , therefore , responsible for providing the basis for appraisal of his/her performance , professional maturity , and likelihood of continued contributions. Consideration for issuance of a continuous contract ( tenure ) begins no later than spring of the fifth year and is completed no later than the sixth year of employment. University guidelines state clearly that \" promotion to professor should not be considered to be forthcoming merely because of years of service to the university. \"\n",
        "\"\n",
        "\n",
        "Query ID: 3232\n",
        "\n",
        "Query Sentence: \"You not only have to have marketing and **promotion** strategies for your business , you also need to be able to communicate them effectively and efficiently so that customers will be attracted to your business and what it offers. Micro and Home Based Businesses can benefit from the marketing and promotion opportunities offered to members by the Melbourne Chapter of Marketing Communications Executives International ( MCEI ) .\"\n",
        "\n",
        "Nearest Neighbor: \"You not only have to have marketing and **promotion** strategies for your business , you also need to be able to communicate them effectively and efficiently so that customers will be attracted to your business and what it offers. Micro and Home Based Businesses can benefit from the marketing and promotion opportunities offered to members by the Melbourne Chapter of Marketing Communications Executives International ( MCEI ) .\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J7_7uLUKQ5uO",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "J7_7uLUKQ5uO",
        "outputId": "5d10634f-f21b-4607-e9d5-f151ea8ed3cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QUERY (target word=promotion):\n",
            "You not only have to have marketing and promotion strategies for your business , you also need to be able to communicate them effectively and efficiently so that customers will be attracted to your business and what it offers. Micro and Home Based Businesses can benefit from the marketing and promotion opportunities offered to members by the Melbourne Chapter of Marketing Communications Executives International ( MCEI ) . \n",
            "NEIGHBORS:\n",
            "- 7. CDC. Perspectives in disease prevention and health promotion update : universal precautions for prevention of transmission of human immunodeficiency virus , hepatitis B virus , and other bloodborne pathogens in health-care settings. MMWR 1988; 38 : 377 -- 382 , 387 -- 8 . \n",
            "- *Find free classified ads that could boost the promotion of your web site. These ads could be seen by other people who you are not targeting for , but may as well be interested in your services . \n",
            "- You not only have to have marketing and promotion strategies for your business , you also need to be able to communicate them effectively and efficiently so that customers will be attracted to your business and what it offers. Micro and Home Based Businesses can benefit from the marketing and promotion opportunities offered to members by the Melbourne Chapter of Marketing Communications Executives International ( MCEI ) . \n",
            "- Promotion Data is a community project for site promotion and web development. As the title suggests , data in relation to articles and news are published regularly. Published articles include those contributed by the community , as well as content included by our Editor. Other areas of interest include a calendar of events and webcasts , featuring established speakers in the field of Search Engine Optimization and Marketing. This site also features a small selection of links , and a tutorial section that we thought our users would find interesting. Members have the option to submit articles , make comments , include resource links , or notify the community of impending events in relation to site promotion and web development. All submitted resources are verified by our Editor before being published . \n",
            "- We partner with other website designers and promoters to provide you comprehensive website design , development and promotion for companies and organizations. We approach website design as a team effort. Through these alliances we 've been able to collaborate together to deliver valuable services and support to our business clients . \n"
          ]
        }
      ],
      "source": [
        "show_nearest_neighbor_sentences(3232, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rxakEsXuQNVr",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rxakEsXuQNVr",
        "outputId": "2c42701d-7896-401c-d286-48f2250e4027"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You not only have to have marketing and promotion strategies for your business , you also need to be able to communicate them effectively and efficiently so that customers will be attracted to your business and what it offers. Micro and Home Based Businesses can benefit from the marketing and promotion opportunities offered to members by the Melbourne Chapter of Marketing Communications Executives International ( MCEI ) . \n"
          ]
        }
      ],
      "source": [
        "i = 3232\n",
        "print(\" \".join(items[i]['sentence']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4yfyWE2pQsXA",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4yfyWE2pQsXA",
        "outputId": "78a45325-4e89-4f96-9a80-e8aec73fe8f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Career ladders establish a pathway for career advancement leading to the full performance level of the position. After initial competition to enter the career ladder , successive promotion is dependent upon ( 1 ) the employee meeting legal and regulatory requirements ( time-in-grade restrictions ) , ( 2 ) the employee 's performance , and ( 3 ) the need for and availability of higher level work within the organization. Employees are not guaranteed promotion once selected for a position within an established career ladder. However , managers are encouraged to foster a work environment that affords individuals assigned to career ladders an equal opportunity to demonstrate their ability to perform at the full performance level . \n"
          ]
        }
      ],
      "source": [
        "i = 436\n",
        "print(\" \".join(items[i]['sentence']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25625f5d-4d32-4603-8034-84b37c6bebea",
      "metadata": {
        "id": "25625f5d-4d32-4603-8034-84b37c6bebea"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "## Attention masking\n",
        "\n",
        "Let's take a closer look at how attention masks affect the output of the masked language model.\n",
        "\n",
        "Consider embedding these two sentences which only differ in one token. We modify the attention mask to ignore this token."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b007828c-6fd8-49fd-8030-a3aaae9b50d2",
      "metadata": {
        "id": "b007828c-6fd8-49fd-8030-a3aaae9b50d2"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Answer this question**: (you can add cells and run code to arrive at the answer)\n",
        "\n",
        "If we were to put these inputs through the BERT model, which of the following would be true? For each, explain why or why not.\n",
        "\n",
        "1. The last layer hidden representation of the target tokens (`cat` and `dog`) would be the same.\n",
        "2. The last layer hidden representation of the third non-special token (`fed` in both cases) would be the same.\n",
        "3. The first layer hidden representation and the last layer hidden representation of each target token (e.g., the first and last layer representation of the `cat` token) would be the same.\n",
        "4. If we mask out everything _but_ the target tokens, the first and last layer hidden representations of each target token would be the same."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93db7478",
      "metadata": {
        "id": "93db7478"
      },
      "source": [
        "1. False. They won't be because they would be treated as two words whose embeddings are such that they can't see any other word in the sentence and since cat and dog are two separate words (different input IDs) they have separate embeddings\n",
        "2. True. Because it would have the same context in both cases where the context \"cat\" or \"dog\" would be masked\n",
        "3. False. Because it would be passing through the different layers through multiple activation functions and normalizations. Even though it won't be combined with other vectors, it would still pass through multiple functions, resulting in different variables\n",
        "4. False. Similar to 3, all the layers would subsequently modify the input from the last layer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d3314fe-0a75-4c6f-8606-ffa74b8912b5",
      "metadata": {
        "id": "3d3314fe-0a75-4c6f-8606-ffa74b8912b5"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "## Upload instructions\n",
        "\n",
        "Upload your `.ipynb` file (with all of the cells executed so that the outputs are visible) to Gradescope."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ivscpyQ7rnpS"
      },
      "id": "ivscpyQ7rnpS",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.3"
    },
    "otter": {
      "OK_FORMAT": true,
      "tests": {}
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "15e23348500c445dbcd745f334490f0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb6b939777484b06800ab17920693bec",
            "placeholder": "​",
            "style": "IPY_MODEL_dbfccb4a0f654330b36262c7b58a508b",
            "value": " 199/199 [00:00&lt;00:00, 556.86it/s, Materializing param=pooler.dense.weight]"
          }
        },
        "272a0fc883be402f8e9d9679b6707eaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aaff426417cd410a83591660b6ea7ed5",
              "IPY_MODEL_570af17f569946beae25aac7be7b5753",
              "IPY_MODEL_15e23348500c445dbcd745f334490f0e"
            ],
            "layout": "IPY_MODEL_30f34eb106294ba48d6ba5f7edc721d9"
          }
        },
        "30f34eb106294ba48d6ba5f7edc721d9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "536a7979abed4203ba0d4ec5d8d8d708": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "570af17f569946beae25aac7be7b5753": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71c9fcb049cf44deb793ac80267904bf",
            "max": 199,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b011cb4cc4934131882b583cdc44ec57",
            "value": 199
          }
        },
        "71c9fcb049cf44deb793ac80267904bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e1622c0603a43cf922b1de4abba80ea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aaff426417cd410a83591660b6ea7ed5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e1622c0603a43cf922b1de4abba80ea",
            "placeholder": "​",
            "style": "IPY_MODEL_536a7979abed4203ba0d4ec5d8d8d708",
            "value": "Loading weights: 100%"
          }
        },
        "b011cb4cc4934131882b583cdc44ec57": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb6b939777484b06800ab17920693bec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbfccb4a0f654330b36262c7b58a508b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}